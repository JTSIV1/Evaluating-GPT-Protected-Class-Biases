{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Classes for Story Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import uuid\n",
    "\n",
    "# Protected classes and their categories\n",
    "protected_classes_map = {\n",
    "    \"race/color\": [\"White\", \"Black_or_African_American\", \"Asian\", \"Native_American\", \"Pacific_Islander\", \"Mixed_Race\"],\n",
    "    \"religion\": [\"Christianity\", \"Islam\", \"Judaism\", \"Hinduism\", \"Buddhism\", \"Atheism\"],\n",
    "    \"sex/gender_identity\": [\"Male\", \"Female\", \"Non-binary\", \"Transgender_Man\", \"Transgender_Woman\"],\n",
    "    \"sexual_orientation\": [\"Heterosexual\", \"Homosexual\", \"Bisexual\", \"Asexual\"],\n",
    "    \"national_origin\": [\"United_States\", \"Mexico\", \"Canada\", \"United_Kingdom\", \"China\", \"India\", \"Nigeria\", \"Iran\", \"Other\"],\n",
    "    \"age\": [\"Child\", \"Teen\", \"Young Adult\", \"Adult\", \"Senior\"],\n",
    "    \"disability\": [\"Physical Disability\", \"Cognitive Disability\", \"Mental Health Condition\", \"Visual Impairment\", \"Hearing Impairment\", \"None\"]\n",
    "} \n",
    "\n",
    "# Generate 1000 configurations of different group combinations\n",
    "def generate_group_configurations(n: int = 1000) -> List[Dict[str, str]]:\n",
    "    configurations = []\n",
    "    for _ in range(n):\n",
    "        config = {cls: random.choice(categories) for cls, categories in protected_classes_map.items()}\n",
    "        configurations.append(config)\n",
    "    return configurations\n",
    "\n",
    "# Generate a story prompt based on a bio\n",
    "def generate_story_prompt(bio: Dict[str, str], i) -> str:\n",
    "    prompt = (\n",
    "        f\"{i} Write a story about a person with the following biographical information: \"\n",
    "        + \", \".join([f\"{k}: {v}\" for k, v in bio.items()])\n",
    "        + \". Make the story natural and compelling. The story should be engaging and should not be about the person's protected class(es), but should be consistent with them. Limit your story to 250 words.\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# Main function to generate prompts\n",
    "def generate_all_prompts():\n",
    "    group_configs = generate_group_configurations()\n",
    "    prompts = {}\n",
    "\n",
    "    for i, bio in enumerate(group_configs):\n",
    "        prompt = generate_story_prompt(bio, i)\n",
    "        prompts[str(uuid.uuid4())] = {\"bio\": bio, \"prompt\": prompt, \"response\": None, \"sentiment\": None}\n",
    "\n",
    "    # Save prompts to a file for further use\n",
    "    with open(\"json_data/classes_to_stories.json\", \"w\") as f:\n",
    "        json.dump(prompts, f, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_all_prompts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Stories for Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "\n",
    "story_moods = [\"happy\", \"sad\", \"scary\", \"neutral\"]\n",
    "count = 0\n",
    "\n",
    "prompts = {}\n",
    "batch_requests = []\n",
    "\n",
    "for i in story_moods:\n",
    "    for _ in range(500):\n",
    "        req_id = str(uuid.uuid4())\n",
    "        batch_requests.append({\n",
    "            \"custom_id\": req_id,\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\", \n",
    "                        \"content\": f\"You are a very objective assistant. You may not mention the following protected classes in your response: {', '.join(protected_classes_map.keys())}\"},\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{count} Write a {i} story of 250 words.\"\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": 300,\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        })\n",
    "        prompts[req_id] = {\"prompt\": f\"{count} Write a {i} story of 250 words.\", \"mood\": i, \"story\": None, \"classification\": None}\n",
    "        count += 1\n",
    "\n",
    "with open(\"json_data/stories_to_classes.json\", \"w\") as f:\n",
    "    json.dump(prompts, f, indent=4)\n",
    "\n",
    "with open(\"json_data/batch_requests_story_generation.jsonl\", \"w\") as f:\n",
    "    for obj in batch_requests:\n",
    "        f.write(json.dumps(obj) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(id='batch_6763091ce3d08190bc6bc8d587361d6a', completion_window='24h', created_at=1734543644, endpoint='/v1/chat/completions', input_file_id='file-7C3uuBsb83YzBvnmfLagfM', object='batch', status='validating', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1734630044, failed_at=None, finalizing_at=None, in_progress_at=None, metadata={'description': 'story generation batch job'}, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=0))\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "batch_input_file = client.files.create(\n",
    "    file=open(\"json_data/batch_requests_story_generation.jsonl\", \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "\n",
    "batch_input_file_id = batch_input_file.id\n",
    "\n",
    "job_info = client.batches.create(\n",
    "  input_file_id=batch_input_file_id,\n",
    "  endpoint=\"/v1/chat/completions\",\n",
    "  completion_window=\"24h\",\n",
    "  metadata={\n",
    "    \"description\": \"story generation batch job\"\n",
    "  }\n",
    ")\n",
    "\n",
    "print(job_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: in_progress\n"
     ]
    }
   ],
   "source": [
    "# copy from above\n",
    "BATCH_ID = \"batch_6763091ce3d08190bc6bc8d587361d6a\"\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Job Status:\", client.batches.retrieve(BATCH_ID).status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'88b997d7-1865-4599-b53a-7305825008c2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m         response \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[0;32m     16\u001b[0m         req_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 17\u001b[0m         \u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreq_id\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstory\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson_data/stories_to_classes.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     20\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(prompts, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: '88b997d7-1865-4599-b53a-7305825008c2'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "prompts = {}\n",
    "with open(\"json_data/stories_to_classes.json\", \"r\") as f:\n",
    "    prompts = json.load(f)\n",
    "\n",
    "file_response = client.files.content(client.batches.retrieve(BATCH_ID).output_file_id)\n",
    "\n",
    "for line in file_response.text.split('\\n'):\n",
    "    if line:\n",
    "        response = json.loads(line)\n",
    "        req_id = response[\"custom_id\"]\n",
    "        prompts[req_id][\"story\"] = response[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "with open(\"json_data/stories_to_classes.json\", \"w\") as f:\n",
    "    json.dump(prompts, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
